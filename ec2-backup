#!/usr/bin/env python

import os
import time
import sys
import ConfigParser
import argparse
import random
import subprocess

class TriggerCleanup(Exception):
    pass
RESOURCES = []

VERBOSE = os.environ.get("EC2_BACKUP_VERBOSE") is not None

"""
In case the user tries to run this script on a system that
does not have boto installed, give them a helpful error.
"""
try:
    import boto
    import boto.ec2
except ImportError:
    print "Missing required dependency: boto"
    print "Please see: https://github.com/boto/boto"
    sys.exit(1)

"""
Produce an information message
"""
def info(msg):
    if VERBOSE:
        print "[INFO] " + msg

"""
Produce a non-fatal error message
"""
def warn(msg):
    print "[WARN] " + msg

"""
Fail with an error message
"""
def fatal(reason, cleanup=True):
    print "[ERROR] " + reason
    if cleanup:
        raise TriggerCleanup()
    else:
        sys.exit(1)

"""
Try and reach a system over ssh
"""
def can_connect(opts, ip):
    info("trying to test ssh connectivity to instance")
    # -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no was presumably added to SSH_OPTS
    # it will either complain about hosts file or ask that you accept/deny new hosts key without these flags
    if run_shell("ssh %s fedora@%s 'echo  > /dev/null' " % (opts, ip))[0] == 0:
        return True
    return False

parser = argparse.ArgumentParser(description="Create backups in the Amazon Cloud")
parser.add_argument('-m', '--method', default='dd')
parser.add_argument('-v', '--volume', default=None)
parser.add_argument('backupdir')
args = parser.parse_args(sys.argv[1:])

# read AWS config file
config = ConfigParser.ConfigParser()
cfg_file_path = os.environ.get('AWS_CONFIG_FILE')

if cfg_file_path is None:
    fatal("No config file defined; Please set 'AWS_CONFIG_FILE' in your environment", cleanup=False)

try:
    with open(cfg_file_path) as cfg_fp:
        config.readfp(cfg_fp)
except IOError:
    fatal("Failed to open config file: %s" % (cfg_file_path), cleanup=False)

# set properties
try:
    ACCESS_KEY = config.get('default', 'aws_access_key_id')
except ConfigParser.NoOptionError:
    fatal("Unable to read access key id. Is 'aws_access_key_id' defined in %s in section [default]?" % (cfg_file_path), cleanup=False)

try:
    SECRET_KEY = config.get('default', 'aws_secret_access_key')
except ConfigParser.NoOptionError:
    fatal("Unable to read secret access key. Is 'aws_secret_access_key' defined in %s in section [default]?" % (cfg_file_path), cleanup=False)

CREDENTIALS = {
    'aws_access_key_id': ACCESS_KEY,
    'aws_secret_access_key': SECRET_KEY
}

# Possible TODO: allow user to specify instance type in AWS_BACKUP_ARGS
KEY_NAME = os.environ.get('EC2_PRIVATE_KEY')
AMI_IMAGE_ID = 'ami-3b361952'
INSTANCE_TYPE = 'm1.medium' # m1.medium'
AWS_REGION = 'us-east-1'
AWS_AZ = 'us-east-1c' # AZ stands for Availability Zone
SECURITY_GROUP = 'ec2-backup-default'
# TODO Ensure that this directory exists + calculate its size
BCKUPSRCDIR = args.backupdir # Directory to backup, should be last command line option
DEVICE = '/dev/sdf'
TMP_DIR = '/tmp'
VOLUME_ID = args.volume # will be None if unspecified
METHOD = args.method
UPDATE_TIMEOUT = 5*60 # 5 minute maximum wait time (in seconds)
SSH_OPTS = os.environ.get('EC2_BACKUP_FLAGS_SSH', "")

FILESYSTEM_MOUNTED = False

"""
AMIs are bound to a region, so we need to
have an AMI ID for every supported region
"""
AMI_IMAGE_IDS = {
        'us-east-1': 'ami-3b361952',
        'us-west-1': 'ami-68e3d32d',
        'us-west-2': 'ami-56771366',
        'eu-west-1': 'ami-3401e843',
        'ap-southeast-1': 'ami-bccd99ee',
        'ap-southeast-2': 'ami-374bd40d',
        'ap-northeast-1': 'ami-7dd7b47c',
        'sa-east': 'ami-6f6ecf72',
}

if METHOD not in ['dd', 'rsync']:
    fatal('invalid method; please specify either dd or rsync using -m/--method', cleanup=False)

def try_connection(region):
    if region not in AMI_IMAGE_IDS:
        info("Skipping unsupported region %s" % (region,))
        return None

    # Create a Connection to AWS
    info("Connecting to %s..." % (region))
    conn = boto.ec2.connect_to_region(region, **CREDENTIALS)

    if conn is None:
        info("Connection failed; troubleshooting...")
        regions = boto.ec2.regions(**CREDENTIALS)
        if not any(r.name == region for r in regions):
            fatal("Invalid region '%s'; Available regions: %s" % (region, ", ".join([r.name for r in regions])))
        fatal("Failed to connect to region: %s" % (region,))

    info("Connected to region: %s" % (region,))
    return conn

def run_shell(cmd):
    info("running shell command: "+cmd)
    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    output, error = process.communicate()
    return_code = process.returncode
    if return_code != 0:
        warn("non-zero return code for \"cmd\": "+error+"\n\nreturn_code="+str(return_code))
    return (return_code, output, error)

def file_exists(cmd, filename):
    (return_code, output, error) = run_shell(cmd)
    if type(output) == type(""):
        output = output.replace("\t", "\n").replace("\r\n", "\n").replace(" ", "\n").split("\n")

    output = [str(x).strip() for x in output]
    #info("checking for files existance: file={0}, files={1}".format(filename, str(output)))
    return (return_code == 0) and (filename in output)

def file_exists_remote(SSH_OPTS, instance, directory, filename):
    cmd = "ssh -t -t {0} fedora@{1} \" ls {2} \" ".format(SSH_OPTS, instance.public_dns_name, directory)
    return file_exists(cmd, filename)

def lookup_volume(vol_id):
    info("Looking up volume: %s" % (vol_id,))

    regions = boto.ec2.regions(**CREDENTIALS)
    info("Trying to locate the volume in the following regions: %s" % (", ".join([r.name for r in regions]),))

    info('Looking up volume with id: "%s"' % (vol_id))
    for region in regions:
        conn = try_connection(region.name)

        if conn is None:
            continue

        try:
            volumes = conn.get_all_volumes(volume_ids=vol_id) #Get all volumes assocated with the current Instance
            volume = volumes[0]

            if volume.status == 'in-use':
                fatal("Volume %s is already in use" % (vol_id,))

            info("Found volume: %s" % (vol_id,))
            info("Configuring availability zone to match existing volume...")

            global AWS_AZ
            global AMI_IMAGE_ID
            AWS_AZ = volume.zone

            AMI_IMAGE_ID = AMI_IMAGE_IDS[region.name]

            info("Using availability zone: %s" % (AWS_AZ))
            return (conn, volume)

        except boto.ec2.connection.EC2Connection.ResponseError:
            conn.close()
            pass

    fatal('Failed to load remote volume with id: "%s"' % (vol_id))
    # NOTREACHED

try:
    """
    If a volume id was given, we have to ensure to that we are operating
    on the region and AZ that is the same as the volume is in.
    """
    if args.volume is not None:
        conn, volume = lookup_volume(VOLUME_ID)

    else:
        conn = try_connection(AWS_REGION)
        volume = None

    """
    Create a KeyPair to use
    Rather than forcing the user to configure their keypair and get it
    from the environment variables, let's just make a new single-use keypair
    for this session. Being as we'll never connect to this instance again,
    we can create the keypair, use it, and then dispose of it.
    """
    KEYPAIRNAME = 'ec2-backup-%s' % (str(random.randint(0, int(10E10))).zfill(10))
    key = conn.create_key_pair(KEYPAIRNAME)
    path_to_key = os.path.join(TMP_DIR, KEYPAIRNAME + '.pem')
    if key.save(TMP_DIR):
        info("Successfully saved KeyPair to: %s" % (path_to_key))
        RESOURCES.append(('keypair', key))
        SSH_OPTS += " -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no "
        SSH_OPTS += "-i " + path_to_key + " "
    else:
        fatal("Unable to save the KeyPair. Are you able to write to %s?" % (path_to_key))

    """
    Create the Security Group to use if it does not exist
    Check if we have the security group that we need for SSH access. If
    not, then create one for the user so they don't have to configure it.
    """
    try:
        info('Checking for existing security group: %s' % (SECURITY_GROUP))
        group = conn.get_all_security_groups(groupnames=SECURITY_GROUP)[0]
        info('Found security group: %s' % (SECURITY_GROUP))

    except conn.ResponseError, e:
        if e.code == 'InvalidGroup.NotFound':
            info('Creating Security Group: %s' % (SECURITY_GROUP))
            group = conn.create_security_group(SECURITY_GROUP, 'Automatically generated by ec2-backup')
            group.authorize('tcp', 22, 22, '0.0.0.0/0')
        else:
            warn('Failed to find a Security Group; trying to use "%s"' % (SECURITY_GROUP))

    """
    Start an EC2 instance
    """
    info("Starting a remote machine...")
    resrv = conn.run_instances(AMI_IMAGE_ID,        \
                placement=AWS_AZ,                   \
                key_name=KEYPAIRNAME,               \
                instance_type=INSTANCE_TYPE,        \
                security_groups=[SECURITY_GROUP])

    # Wait for instance state to be running
    instance = resrv.instances[0]
    RESOURCES.append(('instance', instance))

    # Timeout after waiting too long
    instanceTimeout = 0
    info("Waiting for remote machine to start...")
    while not instance.state == 'running':
        if instanceTimeout >= UPDATE_TIMEOUT:
            raise TriggerCleanup("Machine has not updated despite waiting %s seconds".format(UPDATE_TIMEOUT))
        time.sleep(10)
        info("Checking if the machine is ready...")
        instance.update()
        instanceTimeout += 10

    info('Instance Ready. Instance ID: ' + instance.id)

    initializationTimedOut = 0
    doneInitializingInstance = False
    while initializationTimedOut < 5*60: # 5 minutes
        if doneInitializingInstance:
            info("system initialized")
            break
        instance_statuses_list = conn.get_all_instance_status(instance_ids=[instance.id])
        if len(instance_statuses_list) == 1:
            currentSystemStatus = str(instance_statuses_list[0].system_status)
            currentInstanceStatus = str(instance_statuses_list[0].instance_status)
            info("system status: '%s'   instance status: '%s'" % (currentSystemStatus, currentInstanceStatus))
            if (currentSystemStatus != 'Status:ok') or (currentInstanceStatus != 'Status:ok'):
                info("waiting for system initialization to finish")
                time.sleep(10)
                initializationTimedOut += 10
                instance.update()
            else:
                info("!system status: '%s'   instance status: '%s'" % (currentSystemStatus, currentInstanceStatus))
                time.sleep(10)
                instance.update()
                doneInitializingInstance = True
        else:
            fatal("unable to get system status. # instances found when searching for status is "+len(instance_statuses_list))

    if not doneInitializingInstance:
        fatal("instance initialization timed out")

    if can_connect(SSH_OPTS, instance.ip_address):
        info("was able to connect to ec2 instance")
    else:
        fatal("could not connect to ec2 instance over ssh")

    """
    Mount EBS volume to new instance
    If there was a volume specified, use that. Otherwise, make a new volume.
    """
    if volume is None:
        info("Creating a new volume...")
        # Possible TODO: determine size of volume from source directory
        volume = conn.create_volume(10, instance.placement)

        RESOURCES.append(('volume', volume))

    volumeTimeout = 0
    info("Waiting for the volume to become available...")
    while volume.status != 'available':
        if volumeTimeout >= UPDATE_TIMEOUT:
            raise TriggerCleanup("volume not available despite waiting %s seconds".format(UPDATE_TIMEOUT))
        time.sleep(5)
        info("Checking if the volume is ready...")
        volume.update()
        volumeTimeout += 5

    info("Trying to attach volume to instance")
    if volume.attach(instance.id, DEVICE):
        info("Successfully attached storage volume.")
    else:
        fatal("Failed to attach volume")

    volume.update()
    info('Volume is attached')

    # TODO: if method is rsync, create filesystem on newly attached volume if none exists (/dev/xvdf)
    """
    Steps to perform sync
      http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html
      sudo file -s /dev/xvdf
      /dev/xvdf: data
      ^ if we see this output then we know that there is no filesystem
      make one with sudo mkfs -t ext4 /dev/xvdf
      the file command does not appear to be installed on fedora by default so we may have to change AMI's
      finally, mount file system:
      mkdir -p /mnt/data-store
      mount /dev/xvdf /mnt/data-store

    Resources:
      dd: http://reliablesolutions.blogspot.com/2009/05/implementing-remote-tar-solutions-with.html
      rsync: http://blog.bobbyallen.me/2011/06/10/automating-remote-backups-over-rsync-with-ssh/
    """

    info("volume.attach_data.device is: '"+str(volume.attach_data.device)+"'")
    attach_location = '/dev/xvdf'

    volume_shows_on_fs = 0
    while not file_exists_remote(SSH_OPTS, instance, "/dev", "xvdf"):
        if volume_shows_on_fs >= 500:
            raise TriggerCleanup("volume not available on filesystem despite waiting {0} seconds".format(UPDATE_TIMEOUT))
        else:
            time.sleep(15)
            info("waiting for attached volume to show up on filesystem")
            volume_shows_on_fs += 15
    #time.sleep(5*60)

    # delete these files so that rsync/dd does not run into issues
    run_shell("ssh -t -t {0} fedora@{1} \" rm -f /home/fedora/{{.bashrc,.bash_profile,.profile}} \" ".format(SSH_OPTS, instance.public_dns_name))
    run_shell("ssh -t -t {0} fedora@{1} \" sudo chown -R fedora /dev/xvdf \" ".format(SSH_OPTS, instance.public_dns_name))

    if METHOD == 'rsync':
        info("backing up %s using rsync" % BCKUPSRCDIR)
        if run_shell("ssh -t -t {0} fedora@{1} \"sudo mkdir -p /mnt/data-store\" ".format(SSH_OPTS, instance.public_dns_name))[0] != 0:
            fatal("could not create mount point on instance for rsync")

        check_fs_val = run_shell("ssh -t -t {0} fedora@{1} \"sudo fsck {2} \" ".format(SSH_OPTS, instance.public_dns_name, str(attach_location)))[0]
        if check_fs_val != 0:
            info("creating filesystem on remote ebs volume")
            if run_shell("ssh -t -t {0} fedora@{1} \"sudo mkfs -t ext4 {2} \" ".format(SSH_OPTS, instance.public_dns_name, str(attach_location)))[0] != 0:
                fatal("could not create filesystem on remote ebs volume")
            else:
                info("created filesystem on remote ebs volume")
        else:
            info("detected existing ebs volume filesystem")

        # the following would mount the block device and make fedora the owner of all files
        mountval = run_shell("ssh -t -t {0} fedora@{1} \" sudo mount {2} /mnt/data-store && sudo chown -R fedora /mnt/data-store \"".format(SSH_OPTS, instance.public_dns_name, str(attach_location)))[0]

        FILESYSTEM_MOUNTED = True

        if mountval != 0:
            fatal("could not mount ebs volume")
        else:
            info("ebs volume mounted")

        """
        +++ BEGIN RSYNC  +++
        """

        rsync_perms = run_shell("ssh -t -t {0} fedora@{1} \"echo 'Defaults !requiretty' | sudo tee /etc/sudoers.d/rsync > /dev/null\"".format(SSH_OPTS, instance.public_dns_name))[0]

        if rsync_perms != 0:
            fatal("Could not change permissions for rsync")
        else:
            info("Rsync permissions changed successfully")

        """
        Options explained
          -e Specify the remote shell to use (ssh with options)
          -a Archive mode; recursive and preserve meta info
          -v Verbose output
          -z Compress the data being transferred
          --rsync-path Set the path to the rsync executable on remote host
        """
        rsync_val = run_shell("rsync -e \"ssh {0} \" -avz -vv --delete --rsync-path=\"sudo rsync\" {1} fedora@{2}:/mnt/data-store ".format(SSH_OPTS, BCKUPSRCDIR, instance.public_dns_name))[0]
        if rsync_val != 0:
            fatal("Failed to rsync with remote system volume")
        else:
            info("Successfully ran rsync with remote volume")

        """
        +++ END RSYNC +++
        """

        umount_val = run_shell("ssh -t -t {0} fedora@{1} \" sudo umount /mnt/data-store \"".format(SSH_OPTS, instance.public_dns_name))[0]
        if umount_val != 0:
            fatal("Failed to unmount remote volume from instance")
        else:
            info("Successfully umounted remote volume from instance")
            FILESYSTEM_MOUNTED = False
    else:
        """
        +++ BEGIN DD +++
        """
        info("Backing up %s using dd" % BCKUPSRCDIR)
        # the following should handle dd without any issues
        dd_val = run_shell("tar -cvf - {3} | ssh {0} fedora@{1} \"dd of={2} bs=64k conv=block\"  ".format(SSH_OPTS, instance.public_dns_name, str(attach_location), BCKUPSRCDIR))[0]
        if dd_val != 0:
            fatal("Could not backup directory to remote ebs volume using dd")
        else:
            info("Successfully backed up directory to remote ebs volume using dd")

        """
        +++ END DD +++
        """


# Initiate Rsync
    info('Synchronized directory with AWS')

#    for dir in BCKUPSRCDIR: # we are only backing up a single directory to my knowledge. argument parsing currently accounts for that

# Unmount EBS volume and terminate the instance
    info('Detaching volume from EC2 Instance and Terminating Instance')

    volume.detach()
    volume.update()
    while not volume.status == 'available':
        info("current status of volume is %s" % volume.status)
        time.sleep(10) # Wait for the volume to detatch
        volume.update()

    info('Volume detatched succesfully')

    instance.terminate()
    info('EC2 Instance terminated successfully')

    print volume.id
    sys.exit(0)

    """ NOTREACHED """

except Exception, e:
    # TODO: shouldn't this be warn(e)?
    warn(str(e))

    """ Check if there is anything that needs to be cleaned up """
    if len(RESOURCES) > 0:
        warn("Cleaning up...")

        # TODO: should we look into deleting our custom security group after we are done?

        """
        RESOURCES contains tuples of the things we have allocated and need to
        be freed (deleted/removed) at the end. This is a subset of all the
        things we create in the sunny-day scenario, in case the script fails
        midway through for any reason.
        """
        for key, obj in reversed(RESOURCES):
            try:
                if key == 'keypair':
                    info("Deleting keypair...")
                    if obj.delete():
                        info("Successfully removed old keypair")
                    else:
                        warn("Failed to delete keypair: %s" % (KEYPAIRNAME,))
                    try:
                        os.remove(path_to_key)
                    except:
                        info("Failed to remove private key file: %s" % (path_to_key,))
                elif key == 'instance':
                    # creating an instance automatically creates an ebs volume.
                    # find and delete that ebs volume as well (should be attached).
                    # we should be careful that this part runs after 'volume' as we dont want to accidentally delete the volume we stored data too
                    # TODO: find and delete that volume
                    if obj.state == 'running':
                        info("Terminating instance...")
                        obj.terminate()
                        info("Instance is stopping...")
                    else:
                        info("instance already terminated")
                elif key == 'volume':
                    if obj.status == 'available' or obj.status == 'in-use':
                        if FILESYSTEM_MOUNTED:
                            info("unmounting filesystem")
                            run_shell("ssh -t -t {0} fedora@{1} \" sudo umount /mnt/data-store \"".format(SSH_OPTS, instance.public_dns_name))

                        info("Cleaning up allocated volume...")
                        obj.update()
                        volume_detached = obj.detach()
                        time.sleep(10)
                        obj.update()
                        detach_forced = False

                        """ If detaching the volume failed, use the force """
                        if not volume_detached:
                            info("Forcing volume detachment")
                            detach_forced = obj.detach(force=True)
                            obj.update()
                            time.sleep(10)

                        info("current status of volume is %s" % obj.status)
                        if not volume_detached and not detach_forced:
                            print "could not detach volume from instance"

                        # wait here for it to be detached first. if its still detaching at time of deletion it will crash
                        volumeTimeout = 0
                        info("Waiting for the volume to become available...")

                        # TODO is this already fixed?
                        # fix this. you must wait for some other status
                        info("Checking if the volume is detached...")
                        while obj.status != 'available':
                            if volumeTimeout >= UPDATE_TIMEOUT*5:
                                info("hit volume detach timeout")
                                break
                            time.sleep(5)
                            info("Checking if the volume is detached...")
                            obj.update()
                            info("current status of volume is %s" % obj.status)
                            volumeTimeout += 5
                        info("current volume status: "+obj.status)

                        """
                        We only want to delete the volume on cleanup if we
                        personally created the volume during program execution
                        """
                        if VOLUME_ID is None:
                            try:
                                info("Deleting volume: %s" % (obj.id,))
                                obj.delete()
                            except Exception, vex:
                                warn("Failed while trying to delete volume: %s" % (obj.id,))
                                info(str(vex))

                    else: # volume is not available
                        info("Volume %s not available or in-use" % (obj.id,))

                else: # bogus entry in RESOURCES - this should never happen
                    info("Unrecognized key when cleaning up RESOUCES: key=%s" % key)

                """ Once we're done cleaning up, exit with code 1 to indicate error """
                sys.exit(1)

                """ NOTREACHED """

            except Exception, inner:
                """
                ANYTHING that happens during cleanup should
                be squelched until we're fully done
                """
                warn(str(inner))
                warn("Cleanup step failed, continuing...")
