#!/usr/bin/env python

import os
import time
import sys
import ConfigParser
import argparse
import random

class TriggerCleanup(Exception):
    pass
RESOURCES = []

VERBOSE = os.environ.get("EC2_BACKUP_VERBOSE") is not None

"""
In case the user tries to run this script on a system that
does not have boto installed, give them a helpful error.
"""
try:
    import boto
    from boto.ec2.connection import EC2Connection
except ImportError:
    print "Missing required dependency: boto"
    print "Please see: https://github.com/boto/boto"
    sys.exit(1)

"""
Produce an information message
"""
def info(msg):
    if VERBOSE:
        print "[INFO] " + msg 

"""
Produce a non-fatal error message
"""
def warn(msg):
    print "[WARN] " + msg

"""
Fail with an error message
"""
def fatal(reason, exit_code=1):
    print "[ERROR] " + reason
    raise TriggerCleanup()

"""
Try and reach a system over ssh
"""
def canConnect(opts, ip):
    info("trying to test ssh connectivity to instance")
    # -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no was presumably added to SSH_OPTS
    # it will either complain about hosts file or ask that you accept/deny new hosts key without these flags
    if os.system("ssh %s fedora@%s 'echo  > /dev/null' " % (opts, ip)) == 0:
        return True
    return False

try:
    parser = argparse.ArgumentParser(description="Create backups in the Amazon Cloud")
    parser.add_argument('-m', '--method', default='dd')
    parser.add_argument('-v', '--volume', default=None)
    parser.add_argument('backupdir')
    args = parser.parse_args(sys.argv[1:])
except:
    # TODO Fail
    raise

# read AWS config file
config = ConfigParser.ConfigParser()
cfg_file_path = os.environ.get('AWS_CONFIG_FILE')

if cfg_file_path is None:
    fatal("No config file defined; Please set 'AWS_CONFIG_FILE' in your environment")

try:
    with open(cfg_file_path) as cfg_fp:
        config.readfp(cfg_fp)
except IOError:
    fatal("Failed to open config file: %s" % (cfg_file_path))

# set properties
try:
    ACCESS_KEY = config.get('default', 'aws_access_key_id')
except ConfigParser.NoOptionError:
    fatal("Unable to read access key id. Is 'aws_access_key_id' defined in %s in section [default]?" % (cfg_file_path))

try:
    SECRET_KEY = config.get('default', 'aws_secret_access_key')
except ConfigParser.NoOptionError:
    fatal("Unable to read secret access key. Is 'aws_secret_access_key' defined in %s in section [default]?" % (cfg_file_path))

CREDENTIALS = {
    'aws_access_key_id': ACCESS_KEY,
    'aws_secret_access_key': SECRET_KEY
}

# Possible TODO: allow user to specify instance type in AWS_BACKUP_ARGS
AMI_IMAGE_ID    = 'ami-3b361952' # Fedora
KEY_NAME        = os.environ.get('EC2_PRIVATE_KEY')
INSTANCE_TYPE   = 't1.micro'
AWS_REGION      = 'us-east-1'
AWS_AZ          = 'us-east-1c' # AZ stands for Availability Zone
SECURITY_GROUP  = 'ec2-backup-default'
BCKUPSRCDIR     = args.backupdir # Directory to backup, should be last command line option
DEVICE          = '/dev/sdf'
TMP_DIR         = '/tmp'
VOLUME_ID       = args.volume # will be None if unspecified
METHOD          = args.method
UPDATE_TIMEOUT  = 5*60 # 5 minute maximum wait time (in seconds)
SSH_OPTS        = os.environ.get('EC2_BACKUP_FLAGS_SSH')

if SSH_OPTS is None:
    SSH_OPTS = ''

# TODO: if VOLUME_ID is specified, get zone for volume and set AWS_AZ to that ZONE. we need instance and volume to be in the same region

if METHOD not in ['dd', 'rsync']:
    fatal('invalid method; please specify one of (%s) using -m/--method' % ("|".join(ALLOWED_METHODS)))

try:
    # Create a Connection to AWS
    info("Connecting to %s..." % (AWS_REGION))

    conn = boto.ec2.connect_to_region(AWS_REGION, **CREDENTIALS)

    if conn is None:
        info("Connection fataled; troubleshooting...")
        regions = boto.ec2.regions(**CREDENTIALS)
        if not any(r.name == AWS_REGION for r in regions):
            fatal("Invalid region '%s'; Available regions: %s" % (AWS_REGION, ", ".join([r.name for r in regions])))
        fatal("Failed to initialize EC2 connection")


    """
    Create a KeyPair to use
    Rather than forcing the user to configure their keypair and get it
    from the environment variables, let's just make a new single-use keypair
    for this session. Being as we'll never connect to this instance again,
    we can create the keypair, use it, and then dispose of it.
    """
    KEYPAIRNAME = 'ec2-backup-%s' % (str(random.randint(0, int(10E10))).zfill(10))
    key = conn.create_key_pair(KEYPAIRNAME)
    path_to_key = os.path.join(TMP_DIR, KEYPAIRNAME + '.pem')
    if key.save(TMP_DIR):
        info("Successfully saved KeyPair to: %s" % (path_to_key))
        RESOURCES.append(('keypair', key))
        SSH_OPTS += " -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i " + path_to_key + " "
    else:
        fatal("Unable to save the KeyPair. Are you able to write to %s?" % (path_to_key))

    """
    Create the Security Group to use if it does not exist
    Check if we have the security group that we need for SSH access. If
    not, then create one for the user so they don't have to configure it.
    """
    try:
        info('Checking for existing security group: %s' % (SECURITY_GROUP))
        group = conn.get_all_security_groups(groupnames=SECURITY_GROUP)[0]
        info('Found security group: %s' % (SECURITY_GROUP))

    except conn.ResponseError, e:
        if e.code == 'InvalidGroup.NotFound':
            info('Creating Security Group: %s' % (SECURITY_GROUP))
            group = conn.create_security_group(SECURITY_GROUP, 'Automatically generated by ec2-backup')
            group.authorize('tcp', 22, 22, '0.0.0.0/0')
        else:
            warn('Failed to find a Security Group; trying to use "%s"' % (SECURITY_GROUP))

    """
    Start an EC2 instance
    """
    info("Starting a remote machine...")
    resrv = conn.run_instances(AMI_IMAGE_ID,        \
                placement=AWS_AZ,                   \
                key_name=KEYPAIRNAME,               \
                instance_type=INSTANCE_TYPE,        \
                security_groups=[SECURITY_GROUP])

    # Wait for instance state to be running
    instance = resrv.instances[0]
    RESOURCES.append(('instance', instance))

    # Timeout after waiting too long
    instanceTimeout = 0
    info("Waiting for remote machine to start...")
    while not instance.state == 'running':
        if instanceTimeout >= UPDATE_TIMEOUT:
            raise TriggerCleanup("Machine has not updated despite waiting %s seconds".format(UPDATE_TIMEOUT))
        time.sleep(10)
        info("Checking if the machine is ready...")
        instance.update()
        instanceTimeout += 10

    info('Instance Ready. Instance ID: ' + instance.id)

    initializationTimedOut = 0
    doneInitializingInstance = False
    while initializationTimedOut < 5*60: # 5 minutes
        if doneInitializingInstance:
            info("system initialized")
            break
        instance_statuses_list = conn.get_all_instance_status(instance_ids = [instance.id])
        if(len(instance_statuses_list) == 1):
            currentSystemStatus = instance_statuses_list[0].system_status
            currentInstanceStatus = instance_statuses_list[0].instance_status
            info("system status: '%s'   instance status: '%s'" % (currentSystemStatus, currentInstanceStatus))
            if currentSystemStatus == 'Status:initializing' or currentInstanceStatus == 'Status:initializing':
                info("waiting for system initialization to finish")
                time.sleep(20)
                initializationTimedOut += 20
                instance.update()
            else:
                info("!system status: '%s'   instance status: '%s'" % (currentSystemStatus, currentInstanceStatus))
                time.sleep(20)
                doneInitializingInstance = True
        else:
            fatal("unable to get system status. # instances found when searching for status is "+len(instance_statuses_list))
    
    if not doneInitializingInstance:
        fail("instance initialization timed out")

    if canConnect(SSH_OPTS, instance.ip_address):
        info("was able to connect to ec2 instance")
    else:
        fatal("could not connect to ec2 instance over ssh")

    """
    Mount EBS volume to new instance
    If there was a volume specified, use that. Otherwise, make a new volume.
    """
    if VOLUME_ID is not None:
        info('Looking up volume with id: "%s"' % (VOLUME_ID))
        volumes = conn.get_all_volumes(volume_ids=VOLUME_ID) #Get all volumes assocated with the current Instance
        if len(volumes) != 1:
            fatal('Failed to load remote volume with id: "%s"' % (VOLUME_ID))
        info("Got the volume")
        volume = volumes[0]

        if volume.status == 'in-use':
            fatal("specified volume is already in use")

        # TODO: check if volume placement is different than the instance placement. can only attach if in same region
    else:
        info("Creating a new volume...")
        # Possible TODO: determine size of volume from source directory
        volume = conn.create_volume(10, instance.placement)

        RESOURCES.append(('volume', volume))

        volumeTimeout = 0
        info("Waiting for the volume to become available...")
        while volume.status != 'available':
            if volumeTimeout >= UPDATE_TIMEOUT:
                raise EC2BackupError("volume not available despite waiting %s seconds".format(UPDATE_TIMEOUT))
            time.sleep(5)
            info("Checking if the volume is ready...")
            volume.update()
            volumeTimeout += 5
    
    info("Trying to attach volume to instance")
    if volume.attach(instance.id, DEVICE): #attach volume
        info("Successfully attached storage volume.")
    else:
        fatal("Failed to attach volume")

    info('Volume is attached')

    # TODO: if method is rsync, create filesystem on newly attached volume if none exists (/dev/xvdf)
    # http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html
    # sudo file -s /dev/xvdf
    # /dev/xvdf: data
    # ^ if we see this output then we know that there is no filesystem
    # make one with sudo mkfs -t ext4 /dev/xvdf
    # the file command does not appear to be installed on fedora by default so we may have to change AMI's
    # finally, mount file system: 
    # mkdir -p /mnt/data-store
    # mount /dev/xvdf /mnt/data-store

    # TODO: handle dd: http://reliablesolutions.blogspot.com/2009/05/implementing-remote-tar-solutions-with.html
    # TODO: handle rsync: http://blog.bobbyallen.me/2011/06/10/automating-remote-backups-over-rsync-with-ssh/

    if METHOD == 'rsync':
        info("backing up %s using rsync" % BCKUPSRCDIR)
        if os.system("ssh {0} fedora@{1} \"sudo mkdir -p /mnt/data-store\" ".format(SSH_OPTS, instance.public_dns_name)) != 0:
            fatal("could not create mount point on instance for rsync")
        # TODO: check if you need to create a filesystem on /dev/xvdf using fsck
        # if yes, os.system("ssh {0} fedora@{1} \" sudo mkfs -t ext4 /dev/xvdf \" ".format(SSH_OPTS, instance.public_dns_name) )
        # do check for failure of previous command

        # the following would mount the block device and make fedora the owner of all files
        # os.system("ssh {0} fedora@{1} \" sudo mount /dev/xvdf /mnt/data-store && sudo chown -R fedora /mnt/data-store \"".format(SSH_OPTS, instance.public_dns_name) )
        # do check for failure of previous command

        # do rsync command here
        # os.system("  rsync -avz -e \"ssh {0} \" {1} fedora@{2}:/mnt/data-store ".format(SSH_OPTS, BCKUPSRCDIR, instance.public_dns_name) )
        # check for failure of previous command here

        # os.system("ssh {0} fedora@{1} \" sudo umount /mnt/data-store \"".format(SSH_OPTS, instance.public_dns_name) )
        # check for failure of previous command here
    else:
        info("backing up %s using dd" % BCKUPSRCDIR)
        # the following should handle dd without any issues
        # os.system( "tar -cvf - BCKUPSRCDIR | ssh {0} fedora@{1} \"dd of=/dev/xvdf bs=64k conv=block\"  ".format(SSH_OPTS, instance.public_dns_name) )

# Initiate Rsync
    info('Synchronizing directory with AWS')

#    for dir in BCKUPSRCDIR: # we are only backing up a single directory to my knowledge. argument parsing currently accounts for that
#        """
#        Options explained
#        -e Specify the remote shell to use (ssh with options)
#        -a Archive mode; recursive and preserve meta info
#        -v Verbose output
#        -z Compress the data being transferred
#        --rsync-path Set the path to the rsync executable on remote host
#        """
#        # TODO Refactor so that this is more easily understandable
#        os.system("rsync -e \"ssh {0}\" -avz --delete --rsync-path=\"sudo rsync\" {2} ec2-user@{1}:/mnt/data-store{2}".format(SSH_OPTS, instance.dns_name, backup_dir))

    info('Transfer complete')

# Unmount EBS volume and terminate the instance
    info('Detaching volume from EC2 Instance and Terminating Instance')

# TODO Document
    #os.system("ssh -t -t {0} ec2-user@{1} \"sudo umount /mnt/data-store\"".format(SSH_OPTS, instance.dns_name))

# TODO Error checking + document
    volume.detach()
    volume.update()
    while not volume.status == 'available':
        time.sleep(10) # Wait for the volume to detatch
        volume.update()

    info('Volume detatched succesfully')

# TODO Error checking (?)
    instance.terminate()

    info('EC2 Instance terminated successfully')

except Exception, e:
    # TODO: shouldn't this be warn(e)?
    print e
    if len(RESOURCES) > 0:
        warn("Cleaning up...")

        # TODO: should we look into deleting our custom security group after we are done?

        for key, obj in reversed(RESOURCES):
            try:
                if key == 'keypair':
                    info("Deleting keypair...")
                    if obj.delete():
                        info("Successfully removed old keypair")
                    else:
                        warn("Keypair could not be deleted: %s" % (KEYPAIRNAME,))
                    try:
                        os.remove(path_to_key)
                    except:
                        info("Failed to remove private key file: %s" % (path_to_key,))
                elif key == 'instance':
                    # creating an instance automatically creates an ebs volume.
                    # find and delete that ebs volume as well (should be attached).
                    # we should be careful that this part runs after 'volume' as we dont want to accidentally delete the volume we stored data too
                    # TODO: find and delete that volume
                    if obj.state == 'running':
                        info("Terminating instance...")
                        obj.terminate()
                        info("Instance is stopping...")
                    else:
                        info("instance already terminated")
                elif key == 'volume':
                    if obj.status == 'available' or obj.status == 'in-use':
                        info("Deleting allocated volume...")
                        obj.update()
                        volumeDetached = obj.detach()
                        obj.update()
                        detachForced = False
                        if not volumeDetached:
                            detachForced = obj.detach(force=True)
                            obj.update()

                        if not volumeDetached and not detachForced:
                            print "could not detach volume from instance"

                        # wait here for it to be detached first. if its still detaching at time of deletion it will crash
                        volumeTimeout = 0
                        info("Waiting for the volume to become available...")
                        # fix this. you must wait for some other status 
                        info("Checking if the volume is detached...")
                        while obj.status != 'available':
                            if volumeTimeout >= UPDATE_TIMEOUT*5:
                                break
                            time.sleep(5)
                            info("Checking if the volume is detached...")
                            obj.update()
                            volumeTimeout += 5
                        info("current volume status: "+obj.status)

                        try:
                            if VOLUME_ID is None:
                                # you only want to delete the volume on cleanup if we personally created the volume during program execution
                                obj.delete()
                        except Exception, vex:
                            print "could not delete volume: "
                            print vex
                    else:
                        info("volume not available or in-use")
                else:
                    info("unrecognized key when cleaning up RESOUCES: key=%s" % key)

            except Exception, inner:
                print inner
                warn("Cleanup step failed, continuing...")
