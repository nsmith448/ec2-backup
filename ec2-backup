#!/usr/bin/env python

import os
import time
import sys
import ConfigParser
import argparse
import random

class TriggerCleanup(Exception):
    pass
RESOURCES = []

VERBOSE = os.environ.get("EC2_BACKUP_VERBOSE") is not None

"""
In case the user tries to run this script on a system that
does not have boto installed, give them a helpful error.
"""
try:
    import boto
    from boto.ec2.connection import EC2Connection
except ImportError:
    print "Missing required dependency: boto"
    print "Please see: https://github.com/boto/boto"
    sys.exit(1)

"""
Produce an information message
"""
def info(msg):
    if VERBOSE:
        print "[INFO] " + msg 

"""
Produce a non-fatal error message
"""
def warn(msg):
    print "[WARN] " + msg

"""
Fail with an error message
"""
def fatal(reason, cleanup=True):
    print "[ERROR] " + reason
    if cleanup:
        raise TriggerCleanup()
    else:
        sys.exit(1)

"""
Try and reach a system over ssh
"""
def canConnect(opts, ip):
    info("trying to test ssh connectivity to instance")
    # -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no was presumably added to SSH_OPTS
    # it will either complain about hosts file or ask that you accept/deny new hosts key without these flags
    if os.system("ssh %s fedora@%s 'echo  > /dev/null' " % (opts, ip)) == 0:
        return True
    return False

parser = argparse.ArgumentParser(description="Create backups in the Amazon Cloud")
parser.add_argument('-m', '--method', default='dd')
parser.add_argument('-v', '--volume', default=None)
parser.add_argument('backupdir')
args = parser.parse_args(sys.argv[1:])

# read AWS config file
config = ConfigParser.ConfigParser()
cfg_file_path = os.environ.get('AWS_CONFIG_FILE')

if cfg_file_path is None:
    fatal("No config file defined; Please set 'AWS_CONFIG_FILE' in your environment", cleanup=False)

try:
    with open(cfg_file_path) as cfg_fp:
        config.readfp(cfg_fp)
except IOError:
    fatal("Failed to open config file: %s" % (cfg_file_path), cleanup=False)

# set properties
try:
    ACCESS_KEY = config.get('default', 'aws_access_key_id')
except ConfigParser.NoOptionError:
    fatal("Unable to read access key id. Is 'aws_access_key_id' defined in %s in section [default]?" % (cfg_file_path), cleanup=False)

try:
    SECRET_KEY = config.get('default', 'aws_secret_access_key')
except ConfigParser.NoOptionError:
    fatal("Unable to read secret access key. Is 'aws_secret_access_key' defined in %s in section [default]?" % (cfg_file_path), cleanup=False)

CREDENTIALS = {
    'aws_access_key_id': ACCESS_KEY,
    'aws_secret_access_key': SECRET_KEY
}

# Possible TODO: allow user to specify instance type in AWS_BACKUP_ARGS
KEY_NAME        = os.environ.get('EC2_PRIVATE_KEY')
AMI_IMAGE_ID    = 'ami-3b361952'
INSTANCE_TYPE   = 't1.micro'
AWS_REGION      = 'us-east-1'
AWS_AZ          = 'us-east-1a' # AZ stands for Availability Zone
SECURITY_GROUP  = 'ec2-backup-default'
BCKUPSRCDIR     = args.backupdir # Directory to backup, should be last command line option
DEVICE          = '/dev/sdf'
TMP_DIR         = '/tmp'
VOLUME_ID       = args.volume # will be None if unspecified
METHOD          = args.method
UPDATE_TIMEOUT  = 5*60 # 5 minute maximum wait time (in seconds)
SSH_OPTS        = os.environ.get('EC2_BACKUP_FLAGS_SSH', "")

"""
AMIs are bound to a region, so we need to
have an AMI ID for every supported region
"""
AMI_IMAGE_IDS = {
        'us-east-1': 'ami-3b361952',
        'us-west-1': 'ami-68e3d32d',
        'us-west-2': 'ami-56771366',
        'eu-west-1': 'ami-3401e843',
        'ap-southeast-1': 'ami-bccd99ee',
        'ap-southeast-2': 'ami-374bd40d',
        'ap-northeast-1': 'ami-7dd7b47c',
        'sa-east': 'ami-6f6ecf72',
}

if METHOD not in ['dd', 'rsync']:
    fatal('invalid method; please specify one of (%s) using -m/--method' % ("|".join(ALLOWED_METHODS)), cleanup=False)

def try_connection(region):
    if region not in AMI_IMAGE_IDS:
        info("Skipping unsupported region %s" % (region,))
        return None

    # Create a Connection to AWS
    info("Connecting to %s..." % (region))
    conn = boto.ec2.connect_to_region(region, **CREDENTIALS)

    if conn is None:
        info("Connection failed; troubleshooting...")
        regions = boto.ec2.regions(**CREDENTIALS)
        if not any(r.name == region for r in regions):
            fatal("Invalid region '%s'; Available regions: %s" % (region, ", ".join([r.name for r in regions])))
        fatal("Failed to connect to region: %s" % (region,))

    info("Connected to region: %s" % (region,))
    return conn

def lookup_volume(vol_id):
    info("Looking up volume: %s" % (vol_id,))

    regions = boto.ec2.regions(**CREDENTIALS)
    info("Trying to locate the volume in the following regions: %s" % (", ".join([r.name for r in regions]),))

    info('Looking up volume with id: "%s"' % (vol_id))
    for region in regions:
        conn = try_connection(region.name)

        if conn is None:
            continue

        try:
            volumes = conn.get_all_volumes(volume_ids=vol_id) #Get all volumes assocated with the current Instance
            volume = volumes[0]

            if volume.status == 'in-use':
                fatal("Volume %s is already in use" % (vol_id,))

            info("Found volume: %s" % (vol_id,))
            info("Configuring availability zone to match existing volume...")

            global AWS_AZ
            global AMI_IMAGE_ID
            AWS_AZ = volume.zone

            AMI_IMAGE_ID = AMI_IMAGE_IDS[region.name]

            info("Using availability zone: %s" % (AWS_AZ))
            return (conn, volume)

        except boto.ec2.connection.EC2Connection.ResponseError:
            conn.close()
            pass

    fatal('Failed to load remote volume with id: "%s"' % (vol_id))
    # NOTREACHED

try:
    """
    If a volume id was given, we have to ensure to that we are operating
    on the region and AZ that is the same as the volume is in.
    """
    if args.volume is not None:
        conn, volume = lookup_volume(VOLUME_ID)

    else:
        conn = try_connection(AWS_REGION)

        info("Creating a new volume...")
        # Possible TODO: determine size of volume from source directory
        volume = conn.create_volume(10, instance.placement)

        RESOURCES.append(('volume', volume))

    """
    Create a KeyPair to use
    Rather than forcing the user to configure their keypair and get it
    from the environment variables, let's just make a new single-use keypair
    for this session. Being as we'll never connect to this instance again,
    we can create the keypair, use it, and then dispose of it.
    """
    KEYPAIRNAME = 'ec2-backup-%s' % (str(random.randint(0, int(10E10))).zfill(10))
    key = conn.create_key_pair(KEYPAIRNAME)
    path_to_key = os.path.join(TMP_DIR, KEYPAIRNAME + '.pem')
    if key.save(TMP_DIR):
        info("Successfully saved KeyPair to: %s" % (path_to_key))
        RESOURCES.append(('keypair', key))
        SSH_OPTS += " -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i " + path_to_key + " "
    else:
        fatal("Unable to save the KeyPair. Are you able to write to %s?" % (path_to_key))

    """
    Create the Security Group to use if it does not exist
    Check if we have the security group that we need for SSH access. If
    not, then create one for the user so they don't have to configure it.
    """
    try:
        info('Checking for existing security group: %s' % (SECURITY_GROUP))
        group = conn.get_all_security_groups(groupnames=SECURITY_GROUP)[0]
        info('Found security group: %s' % (SECURITY_GROUP))

    except conn.ResponseError, e:
        if e.code == 'InvalidGroup.NotFound':
            info('Creating Security Group: %s' % (SECURITY_GROUP))
            group = conn.create_security_group(SECURITY_GROUP, 'Automatically generated by ec2-backup')
            group.authorize('tcp', 22, 22, '0.0.0.0/0')
        else:
            warn('Failed to find a Security Group; trying to use "%s"' % (SECURITY_GROUP))

    """
    Start an EC2 instance
    """
    info("Starting a remote machine...")
    resrv = conn.run_instances(AMI_IMAGE_ID,        \
                placement=AWS_AZ,                   \
                key_name=KEYPAIRNAME,               \
                instance_type=INSTANCE_TYPE,        \
                security_groups=[SECURITY_GROUP])

    # Wait for instance state to be running
    instance = resrv.instances[0]
    RESOURCES.append(('instance', instance))

    # Timeout after waiting too long
    instanceTimeout = 0
    info("Waiting for remote machine to start...")
    while not instance.state == 'running':
        if instanceTimeout >= UPDATE_TIMEOUT:
            raise TriggerCleanup("Machine has not updated despite waiting %s seconds".format(UPDATE_TIMEOUT))
        time.sleep(10)
        info("Checking if the machine is ready...")
        instance.update()
        instanceTimeout += 10

    info('Instance Ready. Instance ID: ' + instance.id)

    initializationTimedOut = 0
    doneInitializingInstance = False
    while initializationTimedOut < 5*60: # 5 minutes
        if doneInitializingInstance:
            info("system initialized")
            break
        instance_statuses_list = conn.get_all_instance_status(instance_ids = [instance.id])
        if(len(instance_statuses_list) == 1):
            currentSystemStatus = str(instance_statuses_list[0].system_status)
            currentInstanceStatus = str(instance_statuses_list[0].instance_status)
            info("system status: '%s'   instance status: '%s'" % (currentSystemStatus, currentInstanceStatus))
            if (currentSystemStatus != 'Status:ok') or (currentInstanceStatus != 'Status:ok'):
                info("waiting for system initialization to finish")
                time.sleep(10)
                initializationTimedOut += 10
                instance.update()
            else:
                info("!system status: '%s'   instance status: '%s'" % (currentSystemStatus, currentInstanceStatus))
                time.sleep(10)
                instance.update()
                doneInitializingInstance = True
        else:
            fatal("unable to get system status. # instances found when searching for status is "+len(instance_statuses_list))
    
    if not doneInitializingInstance:
        fail("instance initialization timed out")

    if canConnect(SSH_OPTS, instance.ip_address):
        info("was able to connect to ec2 instance")
    else:
        fatal("could not connect to ec2 instance over ssh")

    """
    Mount EBS volume to new instance
    If there was a volume specified, use that. Otherwise, make a new volume.
    """
    volumeTimeout = 0
    info("Waiting for the volume to become available...")
    while volume.status != 'available':
        if volumeTimeout >= UPDATE_TIMEOUT:
            raise TriggerCleanup("volume not available despite waiting %s seconds".format(UPDATE_TIMEOUT))
        time.sleep(5)
        info("Checking if the volume is ready...")
        volume.update()
        volumeTimeout += 5
    
    info("Trying to attach volume to instance")
    if volume.attach(instance.id, DEVICE): #attach volume
        info("Successfully attached storage volume.")
    else:
        fatal("Failed to attach volume")

    volume.update()
    info('Volume is attached')

    # TODO: if method is rsync, create filesystem on newly attached volume if none exists (/dev/xvdf)
    # http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html
    # sudo file -s /dev/xvdf
    # /dev/xvdf: data
    # ^ if we see this output then we know that there is no filesystem
    # make one with sudo mkfs -t ext4 /dev/xvdf
    # the file command does not appear to be installed on fedora by default so we may have to change AMI's
    # finally, mount file system: 
    # mkdir -p /mnt/data-store
    # mount /dev/xvdf /mnt/data-store

    # TODO: handle dd: http://reliablesolutions.blogspot.com/2009/05/implementing-remote-tar-solutions-with.html
    # TODO: handle rsync: http://blog.bobbyallen.me/2011/06/10/automating-remote-backups-over-rsync-with-ssh/

    info("volume.attach_data.device is: '"+str(volume.attach_data.device)+"'")

    if METHOD == 'rsync':
        info("backing up %s using rsync" % BCKUPSRCDIR)
        if os.system("ssh -t -t {0} fedora@{1} \"sudo mkdir -p /mnt/data-store\" ".format(SSH_OPTS, instance.public_dns_name)) != 0:
            fatal("could not create mount point on instance for rsync")

        checkFileSystemVal = os.system("ssh -t -t {0} fedora@{1} \"sudo fsck {2} \" ".format(SSH_OPTS, instance.public_dns_name, str(volume.attach_data.device)))
        if checkFileSystemVal != 0:
            info("creating filesystem on remote ebs volume")
            if os.system("ssh -t -t {0} fedora@{1} \"sudo mkfs -t ext4 {2} \" ".format(SSH_OPTS, instance.public_dns_name, str(volume.attach_data.device))) != 0:
                os.system("ssh -t -t {0} fedora@{1} \"sudo ls /dev \" ".format(SSH_OPTS, instance.public_dns_name))                
                fatal("could not create filesystem on remote ebs volume")
            else:
                info("created filesystem on remote ebs volume")
        else:
            info("detected existing ebs volume filesystem")


        # the following would mount the block device and make fedora the owner of all files
        mountval = os.system("ssh -t -t {0} fedora@{1} \" sudo mount {2} /mnt/data-store && sudo chown -R fedora /mnt/data-store \"".format(SSH_OPTS, instance.public_dns_name, str(volume.attach_data.device)) )

        if mountval != 0:
            fatal("could not mount ebs volume")
        else:
            info("ebs volume mounted")

        # do rsync command here
        rsyncVal = os.system("  rsync -avz --delete -e \"ssh -t -t {0} \" {1} fedora@{2}:/mnt/data-store ".format(SSH_OPTS, BCKUPSRCDIR, instance.public_dns_name) )
        if rsyncVal != 0:
            fatal("failed to rsync with remote system volume")
        else:
            info("successfully ran rsync with remote volume")

        umountVal = os.system("ssh -t -t {0} fedora@{1} \" sudo umount /mnt/data-store \"".format(SSH_OPTS, instance.public_dns_name) )
        if umountVal != 0:
            fatal("failed to unmount remote volume from instance")
        else:
            info("successfully umounted remote volume from instance")
    else:
        info("backing up %s using dd" % BCKUPSRCDIR)
        # the following should handle dd without any issues
        ddVal = os.system( "tar -cvf - BCKUPSRCDIR | ssh -t -t {0} fedora@{1} \"sudo dd of={2} bs=64k conv=block\"  ".format(SSH_OPTS, instance.public_dns_name, str(volume.attach_data.device)) )
        if ddVal != 0:
            fatal("could not backup directory to remote ebs volume using dd")
        else:
            info("successfully backed up directory to remote ebs volume using dd")

# Initiate Rsync
    info('Synchronized directory with AWS')

#    for dir in BCKUPSRCDIR: # we are only backing up a single directory to my knowledge. argument parsing currently accounts for that
#        """
#        Options explained
#        -e Specify the remote shell to use (ssh with options)
#        -a Archive mode; recursive and preserve meta info
#        -v Verbose output
#        -z Compress the data being transferred
#        --rsync-path Set the path to the rsync executable on remote host
#        """
#        # TODO Refactor so that this is more easily understandable
#        os.system("rsync -e \"ssh {0}\" -avz --delete --rsync-path=\"sudo rsync\" {2} ec2-user@{1}:/mnt/data-store{2}".format(SSH_OPTS, instance.dns_name, backup_dir))

# Unmount EBS volume and terminate the instance
    info('Detaching volume from EC2 Instance and Terminating Instance')

    volume.detach()
    volume.update()
    while not volume.status == 'available':
        info("current status of volume is %s" % volume.status)
        time.sleep(10) # Wait for the volume to detatch
        volume.update()

    info('Volume detatched succesfully')

    instance.terminate()

    info('EC2 Instance terminated successfully')

except Exception, e:
    # TODO: shouldn't this be warn(e)?
    warn(str(e))
    if len(RESOURCES) > 0:
        warn("Cleaning up...")

        # TODO: should we look into deleting our custom security group after we are done?

        for key, obj in reversed(RESOURCES):
            try:
                if key == 'keypair':
                    info("Deleting keypair...")
                    if obj.delete():
                        info("Successfully removed old keypair")
                    else:
                        warn("Keypair could not be deleted: %s" % (KEYPAIRNAME,))
                    try:
                        os.remove(path_to_key)
                    except:
                        info("Failed to remove private key file: %s" % (path_to_key,))
                elif key == 'instance':
                    # creating an instance automatically creates an ebs volume.
                    # find and delete that ebs volume as well (should be attached).
                    # we should be careful that this part runs after 'volume' as we dont want to accidentally delete the volume we stored data too
                    # TODO: find and delete that volume
                    if obj.state == 'running':
                        info("Terminating instance...")
                        obj.terminate()
                        info("Instance is stopping...")
                    else:
                        info("instance already terminated")
                elif key == 'volume':
                    if obj.status == 'available' or obj.status == 'in-use':
                        info("Deleting allocated volume...")
                        obj.update()
                        volumeDetached = obj.detach()
                        time.sleep(10)
                        obj.update()
                        detachForced = False
                        if not volumeDetached:
                            info("forcing volume detachment")
                            detachForced = obj.detach(force=True)
                            obj.update()
                            time.sleep(10)
                        else:
                            info("volume should be been detached")

                        info("current status of volume is %s" % obj.status)

                        if not volumeDetached and not detachForced:
                            print "could not detach volume from instance"

                        # wait here for it to be detached first. if its still detaching at time of deletion it will crash
                        volumeTimeout = 0
                        info("Waiting for the volume to become available...")
                        # fix this. you must wait for some other status 
                        info("Checking if the volume is detached...")
                        while obj.status != 'available':
                            if volumeTimeout >= UPDATE_TIMEOUT*5:
                                break
                            time.sleep(5)
                            info("Checking if the volume is detached...")
                            obj.update()
                            info("current status of volume is %s" % obj.status)
                            volumeTimeout += 5
                        info("current volume status: "+obj.status)

                        try:
                            if VOLUME_ID is None:
                                # you only want to delete the volume on cleanup if we personally created the volume during program execution
                                obj.delete()
                        except Exception, vex:
                            print "could not delete volume: "
                            print vex
                    else:
                        info("volume not available or in-use")
                else:
                    info("unrecognized key when cleaning up RESOUCES: key=%s" % key)

            except Exception, inner:
                warn(str(inner))
                warn("Cleanup step failed, continuing...")
